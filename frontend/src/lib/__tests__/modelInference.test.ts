/**
 * Model inference parity test (Frontend).
 *
 * Tests that ONNX.js inference produces results matching backend PyTorch inference.
 *
 * Requires:
 * - Backend baseline inference data (generated by test_inference_parity.py)
 * - ONNX model exported from PyTorch training
 */

import { describe, it, expect, beforeAll } from "vitest";
import * as ort from "onnxruntime-web";

// Use relative path for test fixtures in vitest
const INFERENCE_BASELINE_PATH = "./src/lib/__tests__/fixtures/inference_baseline.json";

interface InferenceBaseline {
  config: {
    input_dim: number;
    output_dim: number;
    num_test_samples: number;
  };
  test_inputs: number[][];
  expected_outputs: number[][];
  parameter_names: string[];
}

describe("Model Inference Parity", () => {
  let baseline: InferenceBaseline | null = null;
  let session: ort.InferenceSession | null = null;

  beforeAll(async () => {
    // Load baseline data
    try {
      const response = await fetch(INFERENCE_BASELINE_PATH);
      if (response.ok) {
        baseline = await response.json();
      }
    } catch (error) {
      console.warn(
        `Could not load inference baseline at ${INFERENCE_BASELINE_PATH}`,
        error
      );
    }

    // Load ONNX model
    try {
      const modelPath = "/models/model.onnx";
      session = await ort.InferenceSession.create(modelPath, {
        executionProviders: ["wasm"],
      });
    } catch (error) {
      console.warn("Could not load ONNX model", error);
    }
  });

  describe("Model Structure", () => {
    it("should have valid ONNX model loaded", () => {
      if (!session) {
        console.warn("Skipping: ONNX model not available");
        return;
      }

      // Check input/output names
      expect(session.inputNames).toBeDefined();
      expect(session.inputNames.length).toBeGreaterThan(0);

      expect(session.outputNames).toBeDefined();
      expect(session.outputNames.length).toBeGreaterThan(0);
    });
  });

  describe("Inference Execution", () => {
    it("should run inference with valid input", async () => {
      if (!session || !baseline) {
        console.warn("Skipping: model or baseline not available");
        return;
      }

      // Create test input tensor
      const testInput = baseline.test_inputs[0];
      const inputName = session.inputNames[0];

      const tensorInput = new ort.Tensor("float32", new Float32Array(testInput), [
        1,
        baseline.config.input_dim,
      ]);

      const feeds: Record<string, ort.Tensor> = {};
      feeds[inputName] = tensorInput;

      // Run inference
      const results = await session.run(feeds);

      // Check output exists
      expect(Object.keys(results).length).toBeGreaterThan(0);

      const outputName = session.outputNames[0];
      expect(outputName in results).toBe(true);
    });

    it("should produce expected output shape", async () => {
      if (!session || !baseline) {
        console.warn("Skipping: model or baseline not available");
        return;
      }

      const testInput = baseline.test_inputs[0];
      const inputName = session.inputNames[0];

      const tensorInput = new ort.Tensor("float32", new Float32Array(testInput), [
        1,
        baseline.config.input_dim,
      ]);

      const feeds: Record<string, ort.Tensor> = {};
      feeds[inputName] = tensorInput;

      const results = await session.run(feeds);
      const outputName = session.outputNames[0];
      const output = results[outputName] as ort.Tensor;

      // Should output [batch_size, 4]
      expect(output.dims).toEqual([1, 4]);
    });
  });

  describe("Inference Parity", () => {
    it("should match backend inference within tolerance", async () => {
      if (!session || !baseline) {
        console.warn("Skipping: model or baseline not available");
        return;
      }

      const tolerance = 0.01; // 1% tolerance for numerical differences
      const inputName = session.inputNames[0];
      const outputName = session.outputNames[0];

      // Test first sample
      const testInput = baseline.test_inputs[0];
      const expectedOutput = baseline.expected_outputs[0];

      const tensorInput = new ort.Tensor("float32", new Float32Array(testInput), [
        1,
        baseline.config.input_dim,
      ]);

      const feeds: Record<string, ort.Tensor> = {};
      feeds[inputName] = tensorInput;

      const results = await session.run(feeds);
      const output = results[outputName] as ort.Tensor;
      const inferredOutput = Array.from(output.data as Float32Array);

      // Compare outputs element-wise
      expect(inferredOutput).toHaveLength(expectedOutput.length);

      for (let i = 0; i < expectedOutput.length; i++) {
        const expected = expectedOutput[i];
        const actual = inferredOutput[i];
        const error = Math.abs(actual - expected);
        const relError = error / (Math.abs(expected) + 1e-8);

        expect(relError).toBeLessThan(tolerance);
      }
    });

    it("should produce consistent outputs across runs", async () => {
      if (!session || !baseline) {
        console.warn("Skipping: model or baseline not available");
        return;
      }

      const testInput = baseline.test_inputs[0];
      const inputName = session.inputNames[0];
      const outputName = session.outputNames[0];

      // Run inference twice
      const run1 = await runInference(session, inputName, outputName, testInput, baseline.config.input_dim);
      const run2 = await runInference(session, inputName, outputName, testInput, baseline.config.input_dim);

      // Should be identical
      expect(run1).toEqual(run2);
    });

    it("should output valid parameter ranges", async () => {
      if (!session || !baseline) {
        console.warn("Skipping: model or baseline not available");
        return;
      }

      const inputName = session.inputNames[0];
      const outputName = session.outputNames[0];

      // Test all samples
      for (const testInput of baseline.test_inputs) {
        const output = await runInference(
          session,
          inputName,
          outputName,
          testInput,
          baseline.config.input_dim
        );

        const [deltaReal, deltaImag, targetHeight, normalRisk] = output;

        expect(Math.abs(deltaReal)).toBeLessThan(1);
        expect(Math.abs(deltaImag)).toBeLessThan(1);
        expect(Math.abs(targetHeight)).toBeLessThan(10);
        expect(normalRisk).toBeGreaterThanOrEqual(-1);
        expect(normalRisk).toBeLessThanOrEqual(2);
      }
    });
  });
});

/**
 * Helper function to run inference and return output array.
 */
async function runInference(
  session: ort.InferenceSession,
  inputName: string,
  outputName: string,
  input: number[],
  inputDim: number
): Promise<number[]> {
  const tensorInput = new ort.Tensor("float32", new Float32Array(input), [1, inputDim]);

  const feeds: Record<string, ort.Tensor> = {};
  feeds[inputName] = tensorInput;

  const results = await session.run(feeds);
  const output = results[outputName] as ort.Tensor;

  return Array.from(output.data as Float32Array);
}
